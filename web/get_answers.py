import requests
from langchain_community.tools import DuckDuckGoSearchRun
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains import LLMChain
from langchain_core.prompts import PromptTemplate


# ============================================================
# üîπ H√†m g·ªçi model b·∫°n ƒë√£ host qua vLLM serve
# ============================================================
def call_vllm_chat(messages):
    """
    G·ª≠i h·ªôi tho·∫°i t·ªõi model ƒëang host b·∫±ng vLLM serve (OpenAI API format)
    """
    url = "http://localhost:8000/v1/chat/completions"
    payload = {
        "model": "Qwen/Qwen2.5-0.5B-Instruct",
        "messages": messages,
        "max_tokens": 500,
        "temperature": 0.7,
        "top_p": 0.9
    }
    headers = {"Content-Type": "application/json"}
    resp = requests.post(url, json=payload, headers=headers)
    resp.raise_for_status()
    return resp.json()["choices"][0]["message"]["content"].strip()


# ============================================================
# üîπ C√¥ng c·ª• t√¨m ki·∫øm web (DuckDuckGo ‚Äî kh√¥ng c·∫ßn API key)
# ============================================================
search_tool = DuckDuckGoSearchRun()


def search_web(query: str) -> str:
    """T√¨m ki·∫øm nhanh tr√™n web (DuckDuckGo)."""
    try:
        result = search_tool.run(query)
        return result
    except Exception as e:
        return f"L·ªói khi t√¨m ki·∫øm web: {e}"


# ============================================================
# üîπ B·ªô nh·ªõ h·ªôi tho·∫°i ng·∫Øn (4 l∆∞·ª£t g·∫ßn nh·∫•t)
# ============================================================
memory = ConversationBufferWindowMemory(
    k=4,
    memory_key="chat_history",
    return_messages=True
)


# ============================================================
# üîπ Prompt template
# ============================================================
template = """B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¥ng minh, n√≥i ti·∫øng Vi·ªát.
B·∫°n lu√¥n tr·∫£ l·ªùi d·ª±a tr√™n th√¥ng tin m·ªõi nh·∫•t c√≥ th·ªÉ t√¨m ƒë∆∞·ª£c t·ª´ web.

L·ªãch s·ª≠ h·ªôi tho·∫°i g·∫ßn ƒë√¢y:
{chat_history}

Ng∆∞·ªùi d√πng h·ªèi:
{question}

Theo th√¥ng tin m√† t√¥i t√¨m ƒë∆∞·ª£c t·ª´ web:
{web_context}

H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, v√† t·ª± nhi√™n nh·∫•t b·∫±ng ti·∫øng Vi·ªát.
N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√≥i r·∫±ng b·∫°n ch∆∞a t√¨m ƒë∆∞·ª£c th√¥ng tin ch√≠nh x√°c.
"""

prompt = PromptTemplate(
    input_variables=["chat_history", "question", "web_context"],
    template=template
)


# ============================================================
# üîπ Pipeline ch√≠nh: Search ‚Üí Combine ‚Üí G·ªçi model
# ============================================================
def get_response(question: str) -> str:
    """Search web r·ªìi g·ª≠i k·∫øt qu·∫£ k√®m prompt cho model."""
    print(f"üîç ƒêang t√¨m ki·∫øm th√¥ng tin tr√™n web cho c√¢u h·ªèi: {question}")
    web_context = search_web(question)

    # Chu·∫©n b·ªã n·ªôi dung prompt ho√†n ch·ªânh
    full_prompt = prompt.format(
        chat_history=memory.load_memory_variables({})["chat_history"],
        question=question,
        web_context=web_context
    )

    # G·ª≠i sang vLLM model
    messages = [
        {"role": "system", "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI th√¢n thi·ªán, tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát."},
        {"role": "user", "content": full_prompt}
    ]

    print("üß† ƒêang sinh c√¢u tr·∫£ l·ªùi t·ª´ model...")
    answer = call_vllm_chat(messages)

    # L∆∞u h·ªôi tho·∫°i l·∫°i
    memory.chat_memory.add_user_message(question)
    memory.chat_memory.add_ai_message(answer)

    return answer